# üî¨ Architecture Deep Dive

## Loop Holonomy Theory

### What is Holonomy?

In differential geometry, **holonomy** describes the behavior of parallel transport around a closed loop. When you transport a vector around a closed path on a curved surface, it may not return to its original orientation ‚Äì this "rotation" is the holonomy.

### Application to Image Analysis

We adapt this concept to neural embedding spaces:

1. **Embedding Space as Manifold**: CLIP/DINOv2 embeddings live on a high-dimensional manifold (approximately a hypersphere due to L2 normalization)

2. **Degradation as Transport**: Sequential image transformations (JPEG ‚Üí blur ‚Üí scale) define a "path" through embedding space

3. **Holonomy Measurement**: The failure of this path to return to the origin reveals image characteristics

```
         z_0 (original)
        /  \
       /    \
      z_1    z_n (after loop)
       \    /
        \  /
         H = ||z_n - z_0||  ‚Üê This is the holonomy
```

### Why It Works for Deepfake Detection

AI-generated images and real photographs respond differently to degradations:

| Property | Real Images | AI-Generated |
|----------|-------------|--------------|
| **Frequency spectrum** | Natural 1/f noise | Often missing high frequencies |
| **JPEG response** | Predictable DCT behavior | Unusual artifact patterns |
| **Interpolation** | Natural sub-pixel variation | Grid-like patterns |
| **Texture** | Local irregularities | Over-smoothness or repetition |

These differences manifest in the **holonomy signature**.

---

## Feature Extraction Pipeline

### Stage 1: Degradation Loops

We apply 9 carefully designed degradation loops:

```python
LOOPS = [
    ['scale_0.9', 'blur_0.7', 'jpeg_70', 'scale_0.9'],  # Loop 1
    ['blur_0.5', 'jpeg_70', 'blur_0.3', 'identity'],    # Loop 2
    # ... 7 more loops
]
```

Each loop creates a trajectory in embedding space.

### Stage 2: Trajectory Analysis

For each trajectory z_0 ‚Üí z_1 ‚Üí ... ‚Üí z_n:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TRAJECTORY METRICS                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                        ‚îÇ
‚îÇ   H = ||z_n - z_0||           Holonomy (closure gap)   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ   L = Œ£||z_{i+1} - z_i||      Path length              ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ   œÑ = L / H                  Tortuosity (efficiency)   ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ   Œ∫ = mean(1 - cos(Œî_i, Œî_{i+1}))  Curvature          ‚îÇ
‚îÇ                                                        ‚îÇ
‚îÇ   œÉ, Œº, max of step sizes    Statistical moments       ‚îÇ
‚îÇ                                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stage 3: Multi-Scale Analysis

Apply the same analysis at multiple scales:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ GLOBAL (full)   ‚îÇ ‚Üí 63D features
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ TL  ‚îÇ TR  ‚îÇ         ‚îÇ CTR ‚îÇ ‚Üí 5 patches
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   +     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ BL  ‚îÇ BR  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
   Patch Mean ‚Üí 63D features

TOTAL: 126D feature vector
```

---

## Model Architecture Comparison

### V18 (Production SOTA) - 126D

```
Input Image
    ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                ‚îÇ                ‚îÇ
    ‚ñº                ‚ñº                ‚ñº
 GLOBAL           PATCHES          (unused)
 Analysis         (5x)              
    ‚îÇ                ‚îÇ                
    ‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê          
    ‚îÇ           ‚îÇ  MEAN   ‚îÇ          
    ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          
    ‚ñº                ‚ñº                
  [63D]            [63D]             
    ‚îÇ                ‚îÇ                
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                
             ‚îÇ                        
             ‚ñº                        
          [126D]                      
             ‚îÇ                        
       StandardScaler                 
             ‚îÇ                        
        SVM (RBF)                     
             ‚îÇ                        
       Real / Fake                    
```

### Why Not More Complex?

We tested many configurations:

| Config | Dims | AUC | Result |
|--------|------|-----|--------|
| Global only | 63 | 0.878 | Baseline |
| + H2 curvature | 79 | 0.884 | +0.6% |
| + PatchMean | 126 | **0.896** | **+1.8%** ‚úì |
| + PatchMean + H2 | 142 | 0.891 | -0.5% ‚úó |
| + Disagreement | 205 | 0.889 | -0.7% ‚úó |

**Conclusion**: Simpler is better. Additional features add noise.

---

## Encoder Details

### CLIP ViT-L/14

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           CLIP Vision Encoder           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Architecture:  Vision Transformer       ‚îÇ
‚îÇ Patch Size:    14 √ó 14 pixels           ‚îÇ
‚îÇ Input Size:    224 √ó 224 RGB            ‚îÇ
‚îÇ Hidden Dim:    1024                     ‚îÇ
‚îÇ Output Dim:    768                      ‚îÇ
‚îÇ Parameters:    ~300M                    ‚îÇ
‚îÇ Normalization: L2 on output             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Why CLIP?

1. **Pre-trained on 400M image-text pairs** ‚Äì robust representations
2. **Contrastive learning** ‚Äì captures semantic AND visual features
3. **L2 normalized outputs** ‚Äì natural for geometric analysis
4. **No fine-tuning needed** ‚Äì zero-shot transfer

---

## Computational Analysis

### Per-Image Cost

```
Operations per image (V18):
‚îú‚îÄ‚îÄ Global features (1 image, 9 loops)
‚îÇ   ‚îî‚îÄ‚îÄ 9 loops √ó ~5 transforms = 45 CLIP forward passes
‚îÇ
‚îî‚îÄ‚îÄ Patch features (5 patches)
    ‚îî‚îÄ‚îÄ 5 patches √ó 9 loops √ó ~5 transforms = 225 CLIP forward passes
    
TOTAL: ~270 CLIP forward passes per image
       ~50 ms on RTX 3080
```

### Optimization Opportunities

1. **Batch processing**: Collect all transforms, single batch encode
2. **Caching**: Store intermediate embeddings
3. **Loop pruning**: Use fewer loops (top 5 contribute 90% of signal)
4. **Patch reduction**: 3 patches instead of 5

---

## Mathematical Derivations

### Chordal Distance

For L2-normalized vectors a, b on unit sphere:

```
d_chordal(a, b) = ||a - b||_2
                = ‚àö(||a||¬≤ + ||b||¬≤ - 2a¬∑b)
                = ‚àö(1 + 1 - 2¬∑cos(Œ∏))
                = ‚àö(2 - 2¬∑cos(Œ∏))
                = ‚àö(2(1 - cos(Œ∏)))
                = ‚àö(2) ¬∑ ‚àö(1 - cos(Œ∏))
                = 2¬∑sin(Œ∏/2)
```

For small angles: d_chordal ‚âà Œ∏ (geodesic distance on sphere)

### Curvature Estimation

Curvature measures local "bending" of the trajectory:

```
Œ∫_i = angle between consecutive displacements
    = arccos(Œî_i ¬∑ Œî_{i+1} / (||Œî_i|| ¬∑ ||Œî_{i+1}||))

where Œî_i = z_{i+1} - z_i

Mean curvature = (1/n) Œ£ Œ∫_i
```

High curvature ‚Üí trajectory is "wiggly" (often sign of instability)

### Tortuosity

Ratio of path length to direct distance:

```
œÑ = L / H = (Œ£||Œî_i||) / ||z_n - z_0||
```

- œÑ ‚âà 1: Nearly straight path
- œÑ >> 1: Very curved/inefficient path

Real images tend to have higher tortuosity (more stable to degradations).

---

## Empirical Insights

### Feature Importance (from SVM weights)

```
Global Features:
‚îú‚îÄ‚îÄ H (holonomy): 35% contribution
‚îú‚îÄ‚îÄ L/H (tortuosity): 25% contribution  
‚îú‚îÄ‚îÄ max_step: 15% contribution
‚îú‚îÄ‚îÄ mean_step: 10% contribution
‚îî‚îÄ‚îÄ others: 15%

Patch Features:
‚îú‚îÄ‚îÄ Most informative: center patch
‚îú‚îÄ‚îÄ Least informative: corner patches (often background)
‚îî‚îÄ‚îÄ Mean aggregation >> Std aggregation
```

### Per-Loop Contribution

```
Loop 1 (scale-blur-jpeg): 18% ‚Üê Most discriminative
Loop 4 (double-jpeg):     15%
Loop 5 (heavy degrad):    14%
Loop 3 (multi-scale):     12%
...
```

---

## Future Directions

### Potential Improvements

1. **Learnable loops**: Optimize transform sequences end-to-end
2. **Multi-encoder fusion**: Combine CLIP + DINOv2 holonomy
3. **Temporal extension**: Apply to video (frame-to-frame holonomy)
4. **Adversarial robustness**: Test against anti-detection attacks

### Theoretical Questions

1. Why does patch MEAN outperform patch STD?
2. What is the optimal number of degradation steps?
3. Can we derive theoretical bounds on discriminability?
4. Connection to manifold curvature of training data?
