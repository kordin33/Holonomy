{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üî¨ Deepfake Detection Research - Google Colab\n",
        "\n",
        "## Konfiguracja GPU:\n",
        "**Runtime ‚Üí Change runtime type ‚Üí GPU**\n",
        "\n",
        "| GPU | VRAM | Rekomendacja |\n",
        "|-----|------|-------------|\n",
        "| H100 | 80GB | üèÜ Najlepsza (Pro+) |\n",
        "| A100 | 40GB | ‚≠ê ≈öwietna (Pro) |\n",
        "| L4 | 24GB | ‚úÖ Dobra (Pro) |\n",
        "| **T4** | 16GB | ‚úÖ **Free - wybierz to!** |\n",
        "\n",
        "**W≈ÇƒÖcz te≈º: Du≈ºa ilo≈õƒá pamiƒôci RAM** ‚úÖ"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Sprawd≈∫ GPU"
      ],
      "metadata": {
        "id": "gpu_check"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Sprawd≈∫ GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"GPU: {gpu_name}\")\n",
        "    print(f\"VRAM: {vram:.1f} GB\")\n",
        "    \n",
        "    # Rekomendacja batch size\n",
        "    if vram >= 40:\n",
        "        print(\"\\nüéØ Rekomendowany batch size: 64-128\")\n",
        "    elif vram >= 20:\n",
        "        print(\"\\nüéØ Rekomendowany batch size: 32-64\")\n",
        "    elif vram >= 15:\n",
        "        print(\"\\nüéØ Rekomendowany batch size: 16-32\")\n",
        "    else:\n",
        "        print(\"\\nüéØ Rekomendowany batch size: 8-16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Sklonuj Repozytorium"
      ],
      "metadata": {
        "id": "clone_repo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repozytorium GitHub\n",
        "GITHUB_REPO = \"kordin33/DeepFake\"\n",
        "\n",
        "# Sklonuj repo\n",
        "!git clone https://github.com/{GITHUB_REPO}.git\n",
        "\n",
        "# Wejd≈∫ do folderu\n",
        "%cd DeepFake\n",
        "\n",
        "# Poka≈º strukturƒô\n",
        "!ls -la"
      ],
      "metadata": {
        "id": "clone"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Zainstaluj Zale≈ºno≈õci"
      ],
      "metadata": {
        "id": "install_deps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instaluj dodatkowe pakiety\n",
        "!pip install -q scikit-learn tqdm matplotlib seaborn opencv-python-headless\n",
        "!pip install -q wandb  # Weights & Biases\n",
        "!pip install -q datasets huggingface_hub  # Do pobierania danych\n",
        "\n",
        "print(\"\\n‚úÖ Zale≈ºno≈õci zainstalowane!\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ üîê Zaloguj siƒô do Weights & Biases"
      ],
      "metadata": {
        "id": "wandb_login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Zaloguj siƒô do W&B (pojawi siƒô link do wpisania klucza API)\n",
        "wandb.login()\n",
        "\n",
        "print(\"\\n‚úÖ Zalogowano do Weights & Biases!\")"
      ],
      "metadata": {
        "id": "wandb_login_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Konfiguracja Eksperymentu"
      ],
      "metadata": {
        "id": "config"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# KONFIGURACJA - ZMIE≈É TUTAJ\n",
        "# =============================================================================\n",
        "\n",
        "# W&B Project\n",
        "WANDB_PROJECT = \"msc-deepfake-detection\"\n",
        "\n",
        "# Eksperymenty do uruchomienia\n",
        "EXPERIMENT = \"all\"  # \"all\", \"baseline\", \"advanced\", \"ultimate\"\n",
        "\n",
        "# Training\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32  # Dostosuj do GPU (T4: 16-32, A100: 64-128)\n",
        "LEARNING_RATE = 1e-4\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Data\n",
        "MAX_PER_CLASS_A = 5000  # Max samples per class dla datasetu A\n",
        "MAX_PER_CLASS_B = 2000  # Max samples per class dla datasetu B\n",
        "\n",
        "# Augmentation\n",
        "USE_SBI = True  # Self-Blended Images (rekomendowane!)\n",
        "\n",
        "# CUDA\n",
        "USE_COMPILE = True  # torch.compile() - 20-40% speedup\n",
        "\n",
        "# Seed\n",
        "SEED = 42\n",
        "\n",
        "print(f\"üìä Konfiguracja:\")\n",
        "print(f\"   Experiment: {EXPERIMENT}\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Use SBI: {USE_SBI}\")\n",
        "print(f\"   Use compile: {USE_COMPILE}\")"
      ],
      "metadata": {
        "id": "config_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Pobierz Dane z HuggingFace"
      ],
      "metadata": {
        "id": "data_download"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pobierz i przygotuj dane u≈ºywajƒÖc wbudowanego skryptu\n",
        "!python efficientnet_b0_deepfake.py --prepare --data-root ./data \\\n",
        "    --max-per-class-a {MAX_PER_CLASS_A} \\\n",
        "    --max-per-class-b {MAX_PER_CLASS_B}\n",
        "\n",
        "# Sprawd≈∫ dane\n",
        "print(\"\\nüìÅ Struktura danych:\")\n",
        "!find ./data -type d | head -20"
      ],
      "metadata": {
        "id": "download_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Policz pliki w ka≈ºdym folderze\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "data_root = Path(\"./data\")\n",
        "for split_dir in data_root.rglob(\"*\"):\n",
        "    if split_dir.is_dir() and (split_dir / \"fake\").exists():\n",
        "        fake_count = len(list((split_dir / \"fake\").glob(\"*\")))\n",
        "        real_count = len(list((split_dir / \"real\").glob(\"*\")))\n",
        "        print(f\"{split_dir.relative_to(data_root)}: fake={fake_count}, real={real_count}\")"
      ],
      "metadata": {
        "id": "count_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Quick Test"
      ],
      "metadata": {
        "id": "quick_test"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test czy wszystkie modu≈Çy dzia≈ÇajƒÖ\n",
        "!python quick_test.py"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Setup CUDA Optimizations"
      ],
      "metadata": {
        "id": "cuda_setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "from deepfake_research.utils.cuda_utils import (\n",
        "    setup_cuda_optimizations,\n",
        "    print_cuda_memory_stats,\n",
        ")\n",
        "\n",
        "# Setup optimizations\n",
        "cuda_config = setup_cuda_optimizations(\n",
        "    use_compile=USE_COMPILE,\n",
        "    use_cudnn_benchmark=True,\n",
        "    use_tf32=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print_cuda_memory_stats()"
      ],
      "metadata": {
        "id": "cuda_opt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9Ô∏è‚É£ üöÄ Uruchom Eksperymenty z W&B Logging"
      ],
      "metadata": {
        "id": "run_experiments"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uruchom eksperymenty z logowaniem do W&B\n",
        "!python run_experiments.py \\\n",
        "    --experiment {EXPERIMENT} \\\n",
        "    --epochs {EPOCHS} \\\n",
        "    --batch-size {BATCH_SIZE} \\\n",
        "    --lr {LEARNING_RATE} \\\n",
        "    --data-root ./data \\\n",
        "    --output-dir ./experiments \\\n",
        "    --wandb \\\n",
        "    --wandb-project {WANDB_PROJECT} \\\n",
        "    {'--use-sbi' if USE_SBI else ''} \\\n",
        "    {'--compile' if USE_COMPILE else ''} \\\n",
        "    --seed {SEED}"
      ],
      "metadata": {
        "id": "run_exp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîü Alternatywnie: Rƒôczne Trenowanie z Pe≈ÇnƒÖ KontrolƒÖ"
      ],
      "metadata": {
        "id": "manual_train"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from deepfake_research.models.factory import create_model\n",
        "from deepfake_research.data.datasets import create_dataloaders\n",
        "from deepfake_research.training.trainer import Trainer\n",
        "from deepfake_research.training.optimizers import get_optimizer, get_scheduler\n",
        "from deepfake_research.training.losses import DeepfakeLoss\n",
        "from deepfake_research.utils.cuda_utils import compile_model\n",
        "\n",
        "# Lista eksperyment√≥w do uruchomienia\n",
        "MODELS_TO_TRAIN = [\n",
        "    \"baseline_efficientnet\",\n",
        "    \"baseline_vit\", \n",
        "    \"freq_efficientnet\",\n",
        "    \"attention_efficientnet\",\n",
        "    \"hybrid\",\n",
        "    \"ultimate\",\n",
        "]\n",
        "\n",
        "# Lub pojedynczy model:\n",
        "# MODELS_TO_TRAIN = [\"ultimate\"]\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Za≈Çaduj dane\n",
        "print(\"Loading data...\")\n",
        "loaders = create_dataloaders(\n",
        "    data_root=\"./data\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    use_sbi=USE_SBI,\n",
        "    sbi_probability=0.3,\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(loaders['train'].dataset)} samples\")\n",
        "print(f\"Val: {len(loaders['val'].dataset)} samples\")\n",
        "print(f\"Test A: {len(loaders['test_A'].dataset)} samples\")\n",
        "print(f\"Test B: {len(loaders['test_B'].dataset)} samples\")"
      ],
      "metadata": {
        "id": "manual_setup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trenuj ka≈ºdy model z logowaniem do W&B\n",
        "results = {}\n",
        "\n",
        "for model_name in MODELS_TO_TRAIN:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üöÄ Training: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Inicjalizuj W&B run\n",
        "    run = wandb.init(\n",
        "        project=WANDB_PROJECT,\n",
        "        name=model_name,\n",
        "        config={\n",
        "            \"model\": model_name,\n",
        "            \"stage\": \"training\",\n",
        "            \"img_size\": IMG_SIZE,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"lr\": LEARNING_RATE,\n",
        "            \"epochs\": EPOCHS,\n",
        "            \"use_sbi\": USE_SBI,\n",
        "            \"use_compile\": USE_COMPILE,\n",
        "            \"seed\": SEED,\n",
        "        },\n",
        "        reinit=True,  # Pozwala na wiele run√≥w w jednej sesji\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        # Stw√≥rz model\n",
        "        model = create_model(model_name)\n",
        "        \n",
        "        # Kompiluj dla speedup\n",
        "        if USE_COMPILE:\n",
        "            model = compile_model(model, mode=\"default\")\n",
        "        \n",
        "        # Optimizer\n",
        "        optimizer = get_optimizer(model, lr=LEARNING_RATE)\n",
        "        scheduler = get_scheduler(optimizer, epochs=EPOCHS)\n",
        "        criterion = DeepfakeLoss(loss_type='smooth', label_smoothing=0.1)\n",
        "        \n",
        "        # Trainer\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            train_loader=loaders['train'],\n",
        "            val_loader=loaders['val'],\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            criterion=criterion,\n",
        "            device=device,\n",
        "            epochs=EPOCHS,\n",
        "            use_amp=True,\n",
        "            early_stopping=True,\n",
        "            patience=5,\n",
        "            save_dir=f\"./experiments/{model_name}\",\n",
        "            experiment_name=model_name,\n",
        "            use_wandb=True,\n",
        "        )\n",
        "        \n",
        "        # Trenuj\n",
        "        result = trainer.train()\n",
        "        results[model_name] = result\n",
        "        \n",
        "        print(f\"\\n‚úÖ {model_name}: Best Val Acc = {result['best_val_acc']:.4f}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error training {model_name}: {e}\")\n",
        "        \n",
        "    finally:\n",
        "        wandb.finish()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÅ All training completed!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "manual_train_loop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Benchmark i Por√≥wnanie"
      ],
      "metadata": {
        "id": "benchmark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepfake_research.evaluation.benchmark import Benchmark\n",
        "from deepfake_research.evaluation.metrics import MetricsComputer\n",
        "\n",
        "# Wczytaj wytrenowane modele i zbenchmarkuj\n",
        "print(\"Loading trained models for benchmarking...\")\n",
        "\n",
        "# Inicjalizuj W&B dla benchmarku\n",
        "benchmark_run = wandb.init(\n",
        "    project=WANDB_PROJECT,\n",
        "    name=\"benchmark_comparison\",\n",
        "    config={\"stage\": \"benchmark\"},\n",
        "    reinit=True,\n",
        ")\n",
        "\n",
        "test_loaders = {\n",
        "    'test_A': loaders['test_A'],\n",
        "    'test_B': loaders['test_B'],\n",
        "}\n",
        "\n",
        "benchmark = Benchmark(\n",
        "    dataloaders=test_loaders,\n",
        "    device=device,\n",
        "    output_dir=\"./experiments/benchmark\",\n",
        ")\n",
        "\n",
        "# Dodaj ka≈ºdy wytrenowany model do benchmarku\n",
        "from pathlib import Path\n",
        "\n",
        "for model_name in MODELS_TO_TRAIN:\n",
        "    checkpoint_path = Path(f\"./experiments/{model_name}/{model_name}_best.pth\")\n",
        "    if checkpoint_path.exists():\n",
        "        print(f\"\\nBenchmarking {model_name}...\")\n",
        "        model = create_model(model_name)\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model = model.to(device)\n",
        "        \n",
        "        benchmark.add_model(model, model_name)\n",
        "\n",
        "# Print and log comparison\n",
        "benchmark.print_comparison()\n",
        "\n",
        "# Log to W&B\n",
        "comparison = benchmark.compare()\n",
        "for model_name, data in comparison['models'].items():\n",
        "    wandb.log({\n",
        "        f\"{model_name}/mean_accuracy\": data['mean_accuracy'],\n",
        "        f\"{model_name}/mean_auc\": data['mean_auc'],\n",
        "        f\"{model_name}/params_millions\": data['params_millions'],\n",
        "    })\n",
        "\n",
        "wandb.finish()\n",
        "\n",
        "# Save results\n",
        "benchmark.save_results(\"full_benchmark.json\")"
      ],
      "metadata": {
        "id": "benchmark_cell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£2Ô∏è‚É£ Wy≈õwietl Wyniki"
      ],
      "metadata": {
        "id": "results"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Poka≈º raport\n",
        "!cat experiments/benchmark/BENCHMARK_REPORT.md"
      ],
      "metadata": {
        "id": "show_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wy≈õwietl wykresy\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "for img_file in ['cross_dataset_heatmap.png', 'model_comparison.png']:\n",
        "    path = f'experiments/{img_file}'\n",
        "    if os.path.exists(path):\n",
        "        print(f\"\\n{img_file}:\")\n",
        "        display(Image(path))"
      ],
      "metadata": {
        "id": "show_plots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Szczeg√≥≈Çowe wyniki\n",
        "import json\n",
        "\n",
        "with open('experiments/benchmark/full_benchmark.json', 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üèÜ RANKING MODELI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sort by mean accuracy\n",
        "sorted_models = sorted(\n",
        "    results['detailed_results'].items(),\n",
        "    key=lambda x: x[1]['summary']['mean_accuracy'],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "for i, (model_name, data) in enumerate(sorted_models, 1):\n",
        "    summary = data['summary']\n",
        "    medal = \"ü•á\" if i == 1 else \"ü•à\" if i == 2 else \"ü•â\" if i == 3 else f\"{i}.\"\n",
        "    print(f\"\\n{medal} {model_name}\")\n",
        "    print(f\"   Mean Accuracy: {summary['mean_accuracy']:.4f}\")\n",
        "    print(f\"   Mean AUC: {summary['mean_auc']:.4f}\")\n",
        "    print(f\"   Parameters: {summary['params_millions']:.2f}M\")"
      ],
      "metadata": {
        "id": "ranking"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£3Ô∏è‚É£ Zapisz na Google Drive"
      ],
      "metadata": {
        "id": "save_drive"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "# Nazwa folderu z datƒÖ\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "dest_folder = f\"/content/drive/MyDrive/deepfake_results_{timestamp}\"\n",
        "\n",
        "# Kopiuj wyniki\n",
        "shutil.copytree(\"./experiments\", dest_folder)\n",
        "print(f\"\\n‚úÖ Wyniki zapisane do: {dest_folder}\")\n",
        "\n",
        "# Poka≈º co zapisano\n",
        "!ls -la {dest_folder}"
      ],
      "metadata": {
        "id": "save_to_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìä Summary\n",
        "\n",
        "### Links:\n",
        "- **W&B Dashboard**: https://wandb.ai/YOUR_USERNAME/msc-deepfake-detection\n",
        "- **GitHub Repo**: https://github.com/kordin33/DeepFake\n",
        "\n",
        "### Next Steps:\n",
        "1. Przejrzyj wyniki na W&B\n",
        "2. Pobierz najlepszy model z checkpointu\n",
        "3. Dodaj wiƒôcej danych lub eksperyment√≥w\n",
        "4. Przeprowad≈∫ ablation study"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}