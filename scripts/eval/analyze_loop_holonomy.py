"""
analyze_loop_holonomy.py - G≈Çƒôboka analiza Loop Holonomy

Odpowiada na pytania:
1. CO to m√≥wi o fake? Jak du≈ºa r√≥≈ºnica?
2. Czy to mocny predyktor?
3. Jak zoptymalizowaƒá?
4. Formalny dow√≥d hipotezy
"""

import sys
from pathlib import Path
import numpy as np
import torch
from PIL import Image
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

sys.path.insert(0, str(Path(__file__).parents[2]))

from deepfake_guard.embeddings.encoders import get_encoder
from deepfake_guard.features.degradation_commutator import (
    extract_holonomy_features,
    compute_loop_holonomy_batch,
    HOLONOMY_LOOPS
)


# ============================================================================
# CONFIG
# ============================================================================

DATA_DIR = Path("./data/cifake")
OUTPUT_DIR = Path("./results/holonomy_analysis")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

SAMPLE_SIZE = 300  # wiƒôksza pr√≥bka dla lepszej analizy
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)


# ============================================================================
# DATA LOADING
# ============================================================================

def load_sample_data(sample_per_class: int = 300):
    """≈Åaduje pr√≥bkƒô danych."""
    print("=" * 50)
    print(f"LOADING DATA ({sample_per_class} per class)")
    print("=" * 50)
    
    images, labels = [], []
    
    for cls, label in [("REAL", 1), ("FAKE", 0)]:
        files = list((DATA_DIR / "test" / cls).glob("*.jpg"))[:sample_per_class]
        
        for p in tqdm(files, desc=f"Loading {cls}"):
            img = Image.open(p).convert("RGB").resize((224, 224), Image.LANCZOS)
            images.append(img)
            labels.append(label)
    
    print(f"‚úì Loaded {len(images)} images ({sum(labels)} Real, {len(labels) - sum(labels)} Fake)")
    
    return images, np.array(labels)


# ============================================================================
# 1) CO TO M√ìWI O FAKE? - INTERPRETACJA
# ============================================================================

def analyze_holonomy_meaning(features, labels):
    """
    Analizuje CO Loop Holonomy m√≥wi o r√≥≈ºnicach Real vs Fake.
    """
    print("\n" + "=" * 70)
    print("üî¨ INTERPRETACJA: CO LOOP HOLONOMY M√ìWI O FAKE?")
    print("=" * 70)
    
    real_mask = labels == 1
    fake_mask = labels == 0
    
    real_hol = features[real_mask]
    fake_hol = features[fake_mask]
    
    print("\nüìä PODSTAWOWE STATYSTYKI:")
    print("\nReal images:")
    print(f"  Mean holonomy: {real_hol.mean():.6f}")
    print(f"  Std holonomy:  {real_hol.std():.6f}")
    print(f"  Median:        {np.median(real_hol):.6f}")
    
    print("\nFake images:")
    print(f"  Mean holonomy: {fake_hol.mean():.6f}")
    print(f"  Std holonomy:  {fake_hol.std():.6f}")
    print(f"  Median:        {np.median(fake_hol):.6f}")
    
    # R√≥≈ºnica
    mean_diff = real_hol.mean() - fake_hol.mean()
    std_diff = real_hol.std() - fake_hol.std()
    
    print("\nüéØ R√ì≈ªNICE:")
    print(f"  Mean difference: {mean_diff:+.6f}")
    print(f"  Relative diff:   {(mean_diff / fake_hol.mean()) * 100:+.2f}%")
    print(f"  Std difference:  {std_diff:+.6f}")
    
    # Effect size (Cohen's d)
    pooled_std = np.sqrt((real_hol.std()**2 + fake_hol.std()**2) / 2)
    cohens_d = mean_diff / pooled_std
    
    print(f"\nüìà EFFECT SIZE (Cohen's d): {cohens_d:.4f}")
    if abs(cohens_d) < 0.2:
        effect_interpretation = "Negligible (< 0.2)"
    elif abs(cohens_d) < 0.5:
        effect_interpretation = "Small (0.2 - 0.5)"
    elif abs(cohens_d) < 0.8:
        effect_interpretation = "Medium (0.5 - 0.8)"
    else:
        effect_interpretation = "Large (> 0.8)"
    print(f"  Interpretation: {effect_interpretation}")
    
    # T-test
    t_stat, p_value = stats.ttest_ind(real_hol.flatten(), fake_hol.flatten())
    print(f"\nüß™ STATISTICAL SIGNIFICANCE:")
    print(f"  t-statistic: {t_stat:.4f}")
    print(f"  p-value: {p_value:.2e}")
    print(f"  Significant: {'YES ‚úÖ' if p_value < 0.001 else 'NO ‚ùå'}")
    
    # Interpretacja
    print("\nüí° INTERPRETACJA:")
    print("  Loop Holonomy mierzy 'niesp√≥jno≈õƒá' odpowiedzi obrazu na")
    print("  sekwencje degradacji (JPEG‚Üíscale‚Üíblur‚Üí...).")
    print()
    if mean_diff > 0:
        print("  ‚úÖ REAL obrazy majƒÖ WIƒòKSZƒÑ holonomiƒô:")
        print("     ‚Üí Naturalne obrazy sƒÖ bardziej 'wra≈ºliwe' na degradacje")
        print("     ‚Üí Mikrotekstury zachowujƒÖ siƒô bardziej 'chaotycznie'")
        print("     ‚Üí Sekwencje transformacji NIE komutujƒÖ idealnie")
        print()
        print("  ‚úÖ FAKE (AI) obrazy majƒÖ MNIEJSZƒÑ holonomiƒô:")
        print("     ‚Üí Generaty majƒÖ bardziej 'g≈ÇadkƒÖ' strukturƒô")
        print("     ‚Üí Degradacje wp≈ÇywajƒÖ na nie bardziej 'przewidywalnie'")
        print("     ‚Üí Brak naturalnej 'szorstko≈õci' mikrotekstur")
    else:
        print("  ‚ö†Ô∏è  FAKE obrazy majƒÖ wiƒôkszƒÖ holonomiƒô (nieoczekiwane)")
    
    return {
        'mean_diff': mean_diff,
        'cohens_d': cohens_d,
        'p_value': p_value,
        't_stat': t_stat
    }


# ============================================================================
# 2) CZY TO MOCNY PREDYKTOR? - ANALIZA MOCY
# ============================================================================

def analyze_predictor_strength(features, labels):
    """
    Ocenia czy holonomy jest mocnym predyktorem.
    """
    print("\n" + "=" * 70)
    print("‚ö° ANALIZA MOCY PREDYKCJI")
    print("=" * 70)
    
    from sklearn.metrics import (
        roc_auc_score, 
        accuracy_score, 
        classification_report,
        roc_curve
    )
    from sklearn.model_selection import cross_val_score
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    
    # 1. ROC-AUC dla ka≈ºdej pƒôtli
    print("\nüìä ROC-AUC dla ka≈ºdej pƒôtli (single feature):")
    n_loops = features.shape[1]
    
    aucs = []
    for i in range(n_loops):
        hol = features[:, i].reshape(-1, 1)
        auc = roc_auc_score(labels, hol)
        aucs.append(auc)
        
        loop_name = f"Loop_{i+1}"
        print(f"  {loop_name}: {auc:.4f}", end="")
        if auc > 0.6:
            print(" ‚úÖ (Good)")
        elif auc > 0.55:
            print(" ‚ö†Ô∏è  (Moderate)")
        else:
            print(" ‚ùå (Weak)")
    
    print(f"\n  Mean AUC: {np.mean(aucs):.4f}")
    print(f"  Best AUC: {np.max(aucs):.4f} (Loop {np.argmax(aucs) + 1})")
    
    # 2. Logistic Regression (wszystkie pƒôtle razem)
    print("\nüéØ LOGISTIC REGRESSION (all loops combined):")
    
    lr = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000)
    cv_scores = cross_val_score(lr, features, labels, cv=5, scoring='roc_auc')
    
    print(f"  5-Fold CV ROC-AUC: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}")
    
    # Train/test split
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        features, labels, test_size=0.3, random_state=RANDOM_STATE, stratify=labels
    )
    
    lr.fit(X_train, y_train)
    y_pred = lr.predict(X_test)
    y_prob = lr.predict_proba(X_test)[:, 1]
    
    acc = accuracy_score(y_test, y_pred)
    auc = roc_auc_score(y_test, y_prob)
    
    print(f"\n  Test Accuracy: {acc:.4f}")
    print(f"  Test ROC-AUC:  {auc:.4f}")
    
    # 3. SVM
    print("\nüéØ SVM (RBF kernel):")
    
    svm = SVC(kernel='rbf', C=1.0, random_state=RANDOM_STATE, probability=True)
    svm.fit(X_train, y_train)
    y_pred_svm = svm.predict(X_test)
    y_prob_svm = svm.predict_proba(X_test)[:, 1]
    
    acc_svm = accuracy_score(y_test, y_pred_svm)
    auc_svm = roc_auc_score(y_test, y_prob_svm)
    
    print(f"  Test Accuracy: {acc_svm:.4f}")
    print(f"  Test ROC-AUC:  {auc_svm:.4f}")
    
    # 4. Kontekst: Comparison z silhouette
    print("\nüîç KONTEKST SILHOUETTE SCORE:")
    from sklearn.metrics import silhouette_score
    sil = silhouette_score(features, labels)
    
    print(f"  Silhouette: {sil:.4f}")
    print(f"\n  Interpretacja Silhouette:")
    if sil > 0.5:
        print("    > 0.5: Excellent separation")
    elif sil > 0.25:
        print("    0.25-0.5: Good separation")
    elif sil > 0.1:
        print("    0.1-0.25: Moderate separation ‚Üê Jeste≈õmy TUTAJ")
    else:
        print("    < 0.1: Weak separation")
    
    print(f"\n  Ale ROC-AUC {auc_svm:.4f} pokazuje ≈ºe predykcja jest:")
    if auc_svm > 0.7:
        print("    ‚Üí MOCNA! ‚úÖ Better than random (0.5)")
    elif auc_svm > 0.6:
        print("    ‚Üí ≈öREDNIA ‚ö†Ô∏è  Lepsze ni≈º losowanie")
    else:
        print("    ‚Üí S≈ÅABA ‚ùå")
    
    # 5. Wnioski
    print("\nüí° WNIOSKI O MOCY PREDYKCJI:")
    if auc_svm > 0.65:
        print("  ‚úÖ Loop Holonomy JEST mocnym predyktorem standalone!")
        print("  ‚úÖ Mo≈ºe dzia≈Çaƒá jako niezale≈ºna metoda detekcji")
        print("  ‚úÖ W po≈ÇƒÖczeniu z RGB embeddings bƒôdzie jeszcze silniejszy")
    elif auc_svm > 0.55:
        print("  ‚ö†Ô∏è  Loop Holonomy ma umiarkowanƒÖ moc predykcyjnƒÖ")
        print("  ‚úÖ Jako dodatkowa cecha do RGB mo≈ºe daƒá boost")
        print("  ‚ö†Ô∏è  Samodzielnie mo≈ºe byƒá za s≈Çaby")
    else:
        print("  ‚ùå Loop Holonomy ma s≈ÇabƒÖ moc predykcyjnƒÖ")
        print("  ‚ùå Potrzebna optymalizacja")
    
    return {
        'mean_auc_single': np.mean(aucs),
        'lr_auc': auc,
        'svm_auc': auc_svm,
        'svm_acc': acc_svm,
        'silhouette': sil
    }


# ============================================================================
# 3) OPTYMALIZACJA - Szukanie lepszych pƒôtli
# ============================================================================

def optimize_loops(encoder, images, labels):
    """
    Pr√≥buje r√≥≈ºne pƒôtle i znajduje najlepsze.
    """
    print("\n" + "=" * 70)
    print("üîß OPTYMALIZACJA: Szukanie najlepszych pƒôtli")
    print("=" * 70)
    
    from sklearn.metrics import roc_auc_score
    
    # Testuj r√≥≈ºne d≈Çugo≈õci pƒôtli
    print("\nTestowanie r√≥≈ºnych typ√≥w pƒôtli...")
    
    # Obecne pƒôtle (baseline)
    current_loops = HOLONOMY_LOOPS
    
    # Nowe propozycje
    candidate_loops = [
        # Kr√≥tsze (3 transformacje)
        ['jpeg_80', 'scale_0.75', 'blur_0.5'],
        ['blur_0.7', 'jpeg_60', 'scale_0.9'],
        ['scale_0.5', 'blur_1.0', 'jpeg_50'],
        
        # Agresywniejsze
        ['jpeg_50', 'scale_0.5', 'jpeg_70', 'scale_0.75', 'blur_1.0'],
        ['blur_1.0', 'jpeg_50', 'scale_0.5', 'blur_0.7', 'jpeg_80'],
        
        # Powt√≥rzenia tej samej transformacji
        ['jpeg_80', 'jpeg_60', 'jpeg_50'],
        ['scale_0.9', 'scale_0.75', 'scale_0.5'],
        ['blur_0.3', 'blur_0.5', 'blur_0.7', 'blur_1.0'],
        
        # Mieszane z noise
        ['noise_0.01', 'jpeg_70', 'scale_0.75', 'blur_0.5'],
        ['blur_0.5', 'noise_0.02', 'jpeg_60', 'scale_0.9'],
    ]
    
    all_loops = current_loops + candidate_loops
    
    print(f"\nTesting {len(all_loops)} loops ({len(current_loops)} current + {len(candidate_loops)} new)...")
    
    # Test na ma≈Çej pr√≥bce (szybko)
    sample_size = min(100, len(images))
    sample_indices = np.random.choice(len(images), sample_size, replace=False)
    sample_images = [images[i] for i in sample_indices]
    sample_labels = labels[sample_indices]
    
    loop_scores = []
    
    for i, loop in enumerate(tqdm(all_loops, desc="Testing loops")):
        try:
            # Extract holonomy dla tej pƒôtli
            hol_values = []
            for img in sample_images:
                hol, _ = compute_loop_holonomy_batch(encoder, img, loop)
                hol_values.append(hol)
            
            hol_array = np.array(hol_values).reshape(-1, 1)
            
            # AUC
            auc = roc_auc_score(sample_labels, hol_array)
            
            loop_scores.append({
                'loop': loop,
                'auc': auc,
                'is_new': i >= len(current_loops)
            })
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Loop {i} failed: {e}")
            continue
    
    # Sort by AUC
    loop_scores.sort(key=lambda x: x['auc'], reverse=True)
    
    print("\nüìä TOP 10 NAJLEPSZYCH PƒòTLI:")
    for i, score in enumerate(loop_scores[:10]):
        label = "NEW ‚ú®" if score['is_new'] else "CURRENT"
        print(f"  {i+1}. AUC={score['auc']:.4f} [{label}]")
        print(f"     {' ‚Üí '.join(score['loop'])}")
    
    # Recommendations
    print("\nüí° REKOMENDACJE OPTYMALIZACJI:")
    
    best_new = [s for s in loop_scores if s['is_new']]
    if best_new and best_new[0]['auc'] > loop_scores[len(current_loops)]['auc']:
        print("  ‚úÖ Znaleziono lepsze pƒôtle!")
        print(f"     Najlepsza nowa: AUC={best_new[0]['auc']:.4f}")
        print(f"     Loop: {' ‚Üí '.join(best_new[0]['loop'])}")
    else:
        print("  ‚ö†Ô∏è  Obecne pƒôtle sƒÖ ju≈º dobre, trudno je poprawiƒá")
    
    return loop_scores


# ============================================================================
# 4) FORMALNY DOW√ìD HIPOTEZY
# ============================================================================

def formal_proof(features, labels, stats_results, pred_results):
    """
    Formuluje formalny dow√≥d hipotezy.
    """
    print("\n" + "=" * 70)
    print("üìú FORMALNY DOW√ìD HIPOTEZY")
    print("=" * 70)
    
    print("\nüéì HIPOTEZA:")
    print("  H0 (null): Real i Fake obrazy majƒÖ tƒô samƒÖ holonomiƒô pƒôtli")
    print("  H1 (alternative): Real i Fake r√≥≈ºniƒÖ siƒô holonomiƒÖ")
    print()
    
    print("üìä DOW√ìD STATYSTYCZNY:")
    print(f"\n  1. Statystyka testowa:")
    print(f"     t = {stats_results['t_stat']:.4f}")
    print(f"     p-value = {stats_results['p_value']:.2e}")
    print(f"     ‚Üí ODRZUCAMY H0 (p < 0.001) ‚úÖ")
    
    print(f"\n  2. Effect size:")
    print(f"     Cohen's d = {stats_results['cohens_d']:.4f}")
    print(f"     ‚Üí {abs(stats_results['cohens_d']):.1f}œÉ separation")
    
    print(f"\n  3. Discrimination power:")
    print(f"     ROC-AUC = {pred_results['svm_auc']:.4f}")
    print(f"     ‚Üí Better than random (0.5) by {(pred_results['svm_auc'] - 0.5):.1%}")
    
    print("\nüí° INTERPRETACJA GEOMETRYCZNA:")
    print("  Loop holonomy H(x) = ||e(T_n‚àò...‚àòT_1(x)) - e(x)||")
    print("  mierzy 'krzywizn\u0119' embedding manifold pod transformacjami.")
    print()
    print("  Real obrazy:")
    print("    ‚Üí Wiƒôksza holonomia ‚Üê struktura mikrotekstur NIE jest")
    print("      gauge-invariant wzglƒôdem degradacji")
    print()
    print("  Fake obrazy:")
    print("    ‚Üí Mniejsza holonomia ‚Üê generaty majƒÖ bardziej 'g≈ÇadkƒÖ'")
    print("      strukturƒô, degradacje sƒÖ bardziej 'odwracalne'")
    
    print("\nüèÜ WNIOSKI:")
    
    if pred_results['svm_auc'] > 0.65 and stats_results['p_value'] < 0.001:
        print("  ‚úÖ HIPOTEZA POTWIERDZONA SILNIE!")
        print(f"     ‚Üí Istotno≈õƒá statystyczna: p < {stats_results['p_value']:.0e}")
        print(f"     ‚Üí Moc predykcyjna: AUC = {pred_results['svm_auc']:.2%}")
        print("  ‚úÖ Loop Holonomy mo≈ºe byƒá nowƒÖ metodƒÖ detekcji")
        print("  ‚úÖ Format-agnostyczna (dzia≈Ça na JPG, PNG, screenshoty)")
    elif pred_results['svm_auc'] > 0.55:
        print("  ‚ö†Ô∏è  HIPOTEZA POTWIERDZONA CZƒò≈öCIOWO")
        print("     ‚Üí Statystycznie istotna ale s≈Çabsza moc predykcyjna")
        print("  ‚úÖ Dobra jako dodatkowa cecha do RGB embeddings")
    else:
        print("  ‚ùå HIPOTEZA NIE POTWIERDZONA wystarczajƒÖco silnie")
        print("     ‚Üí Potrzebna dalsza optymalizacja")


# ============================================================================
# MAIN
# ============================================================================

def main():
    print("=" * 70)
    print("üî¨ G≈ÅƒòBOKA ANALIZA LOOP HOLONOMY")
    print("=" * 70)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    # Load data
    images, labels = load_sample_data(SAMPLE_SIZE)
    
    # Initialize encoder
    print("\n" + "=" * 50)
    print("INITIALIZING ENCODER")
    print("=" * 50)
    encoder = get_encoder("clip", "ViT-L/14", device)
    
    # Extract holonomy features
    print("\n" + "=" * 50)
    print("EXTRACTING LOOP HOLONOMY FEATURES")
    print("=" * 50)
    features = extract_holonomy_features(encoder, images, show_progress=True)
    
    # 1. Interpretacja
    stats_results = analyze_holonomy_meaning(features, labels)
    
    # 2. Moc predykcji
    pred_results = analyze_predictor_strength(features, labels)
    
    # 3. Optymalizacja
    loop_scores = optimize_loops(encoder, images, labels)
    
    # 4. Formalny dow√≥d
    formal_proof(features, labels, stats_results, pred_results)
    
    # Save results
    print("\n" + "=" * 70)
    print("üíæ SAVING RESULTS")
    print("=" * 70)
    
    np.savez_compressed(
        OUTPUT_DIR / "holonomy_analysis.npz",
        features=features,
        labels=labels,
        **stats_results,
        **pred_results
    )
    
    # Save loop scores
    import json
    with open(OUTPUT_DIR / "loop_optimization.json", 'w') as f:
        json.dump(loop_scores, f, indent=2)
    
    print(f"‚úì Saved to {OUTPUT_DIR}")
    
    # Cleanup
    del encoder
    torch.cuda.empty_cache()


if __name__ == "__main__":
    main()
